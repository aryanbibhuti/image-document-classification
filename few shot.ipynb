{"cells":[{"cell_type":"markdown","metadata":{"id":"RlZu1rxMt77N"},"source":["# For Training and Loading the Pretrained Model on a Fresh Dataset"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"gkwITU5hFTpM","executionInfo":{"status":"ok","timestamp":1669135861907,"user_tz":-330,"elapsed":3,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import imread\n","from keras.layers import Input\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","# import tf.keras.callbacks "]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1669135863556,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"ZyAYm1hIJ5AF","outputId":"994730e5-ad14-4af8-c736-38695f0fe91e"},"outputs":[{"output_type":"stream","name":"stdout","text":["TF version: 2.9.2\n","Hub version: 0.12.0\n","GPU available\n"]}],"source":["print(\"TF version:\", tf.__version__)\n","print(\"Hub version:\", hub.__version__)\n","\n","# Check for GPU\n","print(\"GPU\", \"available\" if tf.config.list_physical_devices(\"GPU\") else \"not available\")"]},{"cell_type":"code","source":["df_e=pd.read_csv(\"efficient-net-logger.csv\")\n","df_e.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"-c9Pycn0bP72","executionInfo":{"status":"ok","timestamp":1669135863557,"user_tz":-330,"elapsed":34,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"d1c16841-88c3-4fb7-e42a-ca0fb7918153"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   epoch  accuracy      loss\n","0      0  0.503875  1.626086\n","1      1  0.577375  1.384375\n","2      2  0.600187  1.310445\n","3      3  0.617562  1.263306\n","4      4  0.627437  1.228566"],"text/html":["\n","  <div id=\"df-33166343-83f6-49c3-ba21-a2269fee33f9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch</th>\n","      <th>accuracy</th>\n","      <th>loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.503875</td>\n","      <td>1.626086</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.577375</td>\n","      <td>1.384375</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.600187</td>\n","      <td>1.310445</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.617562</td>\n","      <td>1.263306</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.627437</td>\n","      <td>1.228566</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-33166343-83f6-49c3-ba21-a2269fee33f9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-33166343-83f6-49c3-ba21-a2269fee33f9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-33166343-83f6-49c3-ba21-a2269fee33f9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["df_m=pd.read_csv(\"mobile-net-logger.csv\")\n","df_m.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"VsqT1YI4ON9N","executionInfo":{"status":"ok","timestamp":1669135863557,"user_tz":-330,"elapsed":30,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"1342f55c-5f4e-49da-e697-1945b9afc737"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   epoch  accuracy      loss\n","0      0  0.508500  1.617241\n","1      1  0.583375  1.352320\n","2      2  0.605937  1.277720\n","3      3  0.619812  1.231731\n","4      4  0.630625  1.198407"],"text/html":["\n","  <div id=\"df-e5565779-b262-4c34-ad48-0d8ff416edc6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch</th>\n","      <th>accuracy</th>\n","      <th>loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.508500</td>\n","      <td>1.617241</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.583375</td>\n","      <td>1.352320</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.605937</td>\n","      <td>1.277720</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.619812</td>\n","      <td>1.231731</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.630625</td>\n","      <td>1.198407</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5565779-b262-4c34-ad48-0d8ff416edc6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e5565779-b262-4c34-ad48-0d8ff416edc6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e5565779-b262-4c34-ad48-0d8ff416edc6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["df_v=pd.read_csv(\"vision-trans-logger.csv\")\n","df_v.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"7cPeS06POS0K","executionInfo":{"status":"ok","timestamp":1669135863557,"user_tz":-330,"elapsed":27,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"64f237d7-2ede-4e6e-c16e-45effd633e9d"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   epoch  accuracy      loss\n","0      0  0.526500  2.873757\n","1      1  0.641625  2.070771\n","2      2  0.683000  1.864125\n","3      3  0.706438  1.846899\n","4      4  0.738750  1.688732"],"text/html":["\n","  <div id=\"df-95b6be2c-531e-41f8-b7dd-edb7736f2c00\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>epoch</th>\n","      <th>accuracy</th>\n","      <th>loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.526500</td>\n","      <td>2.873757</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.641625</td>\n","      <td>2.070771</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.683000</td>\n","      <td>1.864125</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.706438</td>\n","      <td>1.846899</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.738750</td>\n","      <td>1.688732</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-95b6be2c-531e-41f8-b7dd-edb7736f2c00')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-95b6be2c-531e-41f8-b7dd-edb7736f2c00 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-95b6be2c-531e-41f8-b7dd-edb7736f2c00');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["plt.plot(df_e.epoch, df_e.accuracy)\n","# plt.plot(df_v.epoch, df_v.loss)\n","plt.xlabel(\"Number of training epochs\")\n","plt.ylabel(\"Training accuracy\")\n","# plt.ylabel(\"Training loss\")\n","# plt.title(\"Training set loss per epoch for vision transformers\")\n","plt.title(\"Training set accuracy per epoch for efficient net\")\n","plt.savefig('efficient_net_a.png')"],"metadata":{"id":"5eWsAsUFaW-N","colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"status":"ok","timestamp":1669135863558,"user_tz":-330,"elapsed":26,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"05da1975-588a-4a68-8598-845f24f3cdb7"},"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","execution_count":107,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3758,"status":"ok","timestamp":1669139506280,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"9rcTFkleBhQb","outputId":"05e6a81d-df9a-42bf-e684-868bd8ff2148"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}],"source":["# # Running this cell will provide you with a token to link your drive to this notebook\n","from google.colab import drive\n","import sys\n","\n","drive.mount('/content/gdrive/')\n","sys.path.append('/content/gdrive/My Drive/Datathon/')"]},{"cell_type":"code","execution_count":108,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1669139506280,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"wYX_zML-Bvwv","outputId":"8a26b77b-a163-4c78-946e-9272db58ec3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/.shortcut-targets-by-id/18Pjuiby86W8tPsgJuQAMo0AMjzsG0pLw/Datathon\n"]}],"source":["%cd '/content/gdrive/My Drive/Datathon/'"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1669135867967,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"Ay8RDA8iB7lZ","outputId":"5e305808-b59e-44b3-96d8-ddf3c2364b05"},"outputs":[{"output_type":"stream","name":"stdout","text":["'~'\t\t\t\t        mobile_net_l.png\n"," Datathon2.ipynb\t\t        models\n"," datathonindoml-2022.zip\t        models_archit\n"," densenet_a.png\t\t\t       'New Models.ipynb'\n"," dense-net-logger.csv\t\t        predicted_label_3.csv\n"," densenet_l.png\t\t\t        predicted_label.csv\n"," drive\t\t\t\t       'rekhani LSTM.ipynb'\n"," efficient_net_a.png\t\t        resnet_a.png\n"," efficient-net-logger.csv\t        res-net-logger.csv\n"," efficient_net_l.png\t\t        resnet_l.png\n","'few shot.ipynb'\t\t        sample_submission.csv\n","'for RESNETS.ipynb'\t\t        train\n"," full_np_array.npy\t\t        train_labels.csv\n"," kaggle-indoml-submission.csv\t        validation\n"," kaggle-indoml-submission-model-2.csv   vision_trans_a.png\n"," logs\t\t\t\t       'Vision Transformers.ipynb'\n"," mobile_net_a.png\t\t        vision-trans-logger.csv\n"," mobile-net-logger.csv\t\t        vision_trans_l.png\n"," mobile-net-logger.gsheet\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"tO79ce92J6hH","executionInfo":{"status":"ok","timestamp":1669135867967,"user_tz":-330,"elapsed":6,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["train_labels_csv = pd.read_csv(\"train_labels.csv\")"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"fDXtkZfjKF-H","executionInfo":{"status":"ok","timestamp":1669135867967,"user_tz":-330,"elapsed":6,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["labels = train_labels_csv[\"label\"].to_numpy() # convert labels column to NumPy array (from Training Dataset)\n","# Finding the unique labels\n","unique_labels = np.unique(labels)\n","# Turn every label into a boolean array\n","boolean_labels = [label == np.array(unique_labels) for label in labels]\n"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"2yeVv3pMKVWM","executionInfo":{"status":"ok","timestamp":1669135889042,"user_tz":-330,"elapsed":21081,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# Create pathnames from image ID's\n","train_path = \"train/train/\"\n","filenames = [train_path + str(fname) + \".jpeg\" for fname in train_labels_csv[\"id\"]]      # Fetching training files' IDs from train_labels_csv\n","\n","val_path = \"validation/validation/\"\n","val_filenames = [val_path + str(fname) for fname in os.listdir(val_path)]       # Fetching Validation files' IDs from the validation set"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"wt7g4P4qNa2W","executionInfo":{"status":"ok","timestamp":1669135889042,"user_tz":-330,"elapsed":19,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["X_test = filenames[4000:5000]\n","y_test = boolean_labels[4000:5000]"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"jHQJsrvo5ndK","executionInfo":{"status":"ok","timestamp":1669135889043,"user_tz":-330,"elapsed":19,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# Setup X & y variables\n","X = filenames[0:4000]\n","y = boolean_labels[0:4000]"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"dGGhJS5FKbVl","executionInfo":{"status":"ok","timestamp":1669135889043,"user_tz":-330,"elapsed":18,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# Define image size\n","IMG_SIZE = 224\n","\n","def process_image(image_path):\n","  \"\"\"\n","  Takes an image file path and turns it into a Tensor.\n","  \"\"\"\n","  # Read in image file\n","  image = tf.io.read_file(image_path)\n","  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)\n","  image = tf.image.decode_jpeg(image, channels=3)\n","  # Convert the colour channel values from 0-225 values to 0-1 values\n","  image = tf.image.convert_image_dtype(image, tf.float32)\n","  # Resize the image to our desired size (224, 244)\n","  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n","  return image"]},{"cell_type":"code","source":["# Define image size\n","IMG_SIZE = 256\n","\n","def process_image_2(image_path):\n","  \"\"\"\n","  Takes an image file path and turns it into a Tensor.\n","  \"\"\"\n","  # Read in image file\n","  image = tf.io.read_file(image_path)\n","  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)\n","  image = tf.image.decode_jpeg(image, channels=3)\n","  # Convert the colour channel values from 0-225 values to 0-1 values\n","  image = tf.image.convert_image_dtype(image, tf.float32)\n","  # Resize the image to our desired size (224, 244)\n","  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n","  return image"],"metadata":{"id":"GxFWnCVsULgv","executionInfo":{"status":"ok","timestamp":1669136976074,"user_tz":-330,"elapsed":3,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","execution_count":59,"metadata":{"id":"NJEGS3j_Kk4N","executionInfo":{"status":"ok","timestamp":1669135889043,"user_tz":-330,"elapsed":17,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# Create a simple function to return a tuple (image, label)\n","def get_image_label(image_path, label):\n","  \"\"\"\n","  Takes an image file path name and the associated label,\n","  processes the image and returns a tuple of (image, label).\n","  \"\"\"\n","  image = process_image(image_path)\n","  return image, label"]},{"cell_type":"code","source":["# Create a simple function to return a tuple (image, label)\n","def get_image_label_2(image_path, label):\n","  \"\"\"\n","  Takes an image file path name and the associated label,\n","  processes the image and returns a tuple of (image, label).\n","  \"\"\"\n","  image = process_image_2(image_path)\n","  return image, label"],"metadata":{"id":"UlmWAf8yUSi6","executionInfo":{"status":"ok","timestamp":1669136996212,"user_tz":-330,"elapsed":6,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","execution_count":84,"metadata":{"id":"7hqzeHeIKmc9","executionInfo":{"status":"ok","timestamp":1669136341714,"user_tz":-330,"elapsed":3,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# Define the batch size, 32 is a good default\n","BATCH_SIZE=32\n","# Create a function to turn data into batches\n","def create_data_batches(x, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):\n","  \"\"\"\n","  Creates batches of data out of image (x) and label (y) pairs.\n","  Shuffles the data if it's training data but doesn't shuffle it if it's validation data.\n","  Also accepts test data as input (no labels).\n","  \"\"\"\n","  # If the data is a training dataset, we shuffle it\n","  print(\"Creating training data batches...\")\n","  # Turn filepaths and labels into Tensors\n","  data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths\n","                                            tf.constant(y))) # labels\n","  \n","\n","  # Create (image, label) tuples (this also turns the image path into a preprocessed image)\n","  data = data.map(get_image_label)\n","\n","    # Turn the data into batches\n","  data_batch = data.batch(BATCH_SIZE)\n","  return data_batch"]},{"cell_type":"code","source":["# Define the batch size, 32 is a good default\n","BATCH_SIZE=32\n","# Create a function to turn data into batches\n","def create_data_batches_2(x, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):\n","  \"\"\"\n","  Creates batches of data out of image (x) and label (y) pairs.\n","  Shuffles the data if it's training data but doesn't shuffle it if it's validation data.\n","  Also accepts test data as input (no labels).\n","  \"\"\"\n","  # If the data is a training dataset, we shuffle it\n","  print(\"Creating training data batches...\")\n","  # Turn filepaths and labels into Tensors\n","  data = tf.data.Dataset.from_tensor_slices((tf.constant(x), # filepaths\n","                                            tf.constant(y))) # labels\n","  \n","\n","  # Create (image, label) tuples (this also turns the image path into a preprocessed image)\n","  data = data.map(get_image_label_2)\n","\n","    # Turn the data into batches\n","  data_batch = data.batch(BATCH_SIZE)\n","  return data_batch"],"metadata":{"id":"45Bx7SuVUYXd","executionInfo":{"status":"ok","timestamp":1669137018340,"user_tz":-330,"elapsed":428,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"execution_count":92,"outputs":[]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1669135889044,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"i_EFOCFt1-6q","outputId":"da1b99c8-13fe-4ff0-823f-1c33f43785df"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4000"]},"metadata":{},"execution_count":61}],"source":["len(X)"]},{"cell_type":"code","execution_count":62,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1669135889044,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"E95RHTk_Kn3d"},"outputs":[],"source":["# Turn full training data in a data batch\n","\n","# full_data = create_data_batches(X, y, 4000)"]},{"cell_type":"code","source":["full_data_test = create_data_batches(X_test, y_test, 32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FpEm-Mh_LYj8","executionInfo":{"status":"ok","timestamp":1669136347364,"user_tz":-330,"elapsed":520,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"92d2bdd8-61aa-499d-b315-98039f1a8d34"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating training data batches...\n"]}]},{"cell_type":"code","execution_count":86,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":479,"status":"ok","timestamp":1669136356808,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"DhhLA3xoKyuU","outputId":"410bbd9a-9095-44ed-c744-e20ac5cd3a56"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 16), dtype=tf.bool, name=None))>"]},"metadata":{},"execution_count":86}],"source":["full_data_test"]},{"cell_type":"code","source":["full_data_test_2= create_data_batches(X_test, y_test, 32)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cEY0Mr89UgdS","executionInfo":{"status":"ok","timestamp":1669137052568,"user_tz":-330,"elapsed":6,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"6c8bacdd-0d78-4182-9673-6f328f545e0c"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating training data batches...\n"]}]},{"cell_type":"code","source":["full_data_test_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOtwTYbgUjw8","executionInfo":{"status":"ok","timestamp":1669137060519,"user_tz":-330,"elapsed":14,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"c7ed11a6-05b8-4f05-f884-00b2a7adc7a0"},"execution_count":94,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<BatchDataset element_spec=(TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 16), dtype=tf.bool, name=None))>"]},"metadata":{},"execution_count":94}]},{"cell_type":"code","execution_count":65,"metadata":{"id":"1JQSjm4q_1Et","executionInfo":{"status":"ok","timestamp":1669135891223,"user_tz":-330,"elapsed":18,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# for image, label in full_data.map:\n","#   print(image.shape, label.shape)"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"J4G6G1QOKo6N","executionInfo":{"status":"ok","timestamp":1669135891224,"user_tz":-330,"elapsed":19,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# Setup input shape to the model\n","INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # batch, height, width, colour channels\n","\n","# Setup output shape of the model\n","OUTPUT_SHAPE = len(unique_labels) # number of unique labels\n","\n","# Setup model URL from TensorFlow Hub\n","# MODEL_URL = \"https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5\"\n","# MODEL_URL =\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b0/classification/2\"\n","MODEL_URL =\"https://tfhub.dev/google/supcon/resnet_v1_200/imagenet/classification/1\"\n"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"TYIutERlKrIH","executionInfo":{"status":"ok","timestamp":1669135891224,"user_tz":-330,"elapsed":18,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# we will build the model using the Keras API\n","\n","def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):\n","  print(\"Building the model with:\", MODEL_URL)\n","\n","  # Setup the model layers\n","  model = tf.keras.Sequential([\n","    hub.KerasLayer(MODEL_URL), # Layer 1 (input layer)\n","    tf.keras.layers.Dense(units=OUTPUT_SHAPE, \n","                          activation=\"softmax\") # Layer 2 (output layer). Softmax will predict the probabilities for each class for each image\n","  ])\n","\n","  # Compile the model\n","  model.compile(\n","      loss=tf.keras.losses.CategoricalCrossentropy(), # Our model wants to reduce this (how wrong its guesses are)\n","      optimizer=tf.keras.optimizers.Adam(), # An optimizer helping our model how to improve its guesses\n","      metrics=[\"accuracy\"] # We'd like this to go up\n","  )\n","\n","  # Build the model\n","  model.build(INPUT_SHAPE) # Let the model know what kind of inputs it'll be getting\n","  \n","  return model"]},{"cell_type":"markdown","metadata":{"id":"TQ5fZFDQ4gw5"},"source":["## Creating the Model 2 for Full data Training"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"D0Mvji6nKscJ","executionInfo":{"status":"ok","timestamp":1669135891225,"user_tz":-330,"elapsed":19,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# # # Instantiate a new model for training on the full dataset\n","# full_model2 = create_model()\n","# full_model2.summary()"]},{"cell_type":"code","execution_count":69,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1669135891225,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"},"user_tz":-330},"id":"WNLYMgjtuhqA"},"outputs":[],"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","import datetime\n","\n","# Create a function to build a TensorBoard callback\n","def create_tensorboard_callback():\n","  # Create a log directory for storing TensorBoard logs\n","  logdir = os.path.join(\"logs\",\n","                        # Make it so the logs get tracked whenever we run an experiment\n","                        datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","  return tf.keras.callbacks.TensorBoard(logdir)"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"fnhQKzX8KuM8","executionInfo":{"status":"ok","timestamp":1669135891225,"user_tz":-330,"elapsed":18,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# Create full model callbacks\n","\n","# TensorBoard callback\n","full_model_tensorboard = create_tensorboard_callback()\n","\n","# Early stopping callback\n","# Note: No validation set when training on all the data, so we monitor only training accuracy\n","full_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"accuracy\",\n","                                                             patience=4)\n","\n","logger= tf.keras.callbacks.CSVLogger(\"few-shot-logger.csv\")"]},{"cell_type":"code","execution_count":71,"metadata":{"id":"5HVZ9qF5MrvZ","executionInfo":{"status":"ok","timestamp":1669135891226,"user_tz":-330,"elapsed":19,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["def save_model(model, suffix=None):\n","  \"\"\"\n","  Saves a given model in a models directory and appends a suffix (str)\n","  for clarity and reuse.\n","  \"\"\"\n","  # Create model directory with current time\n","  modeldir = os.path.join(\"models_archit\",\n","                          datetime.datetime.now().strftime(\"%Y%m%d-%H%M%s\"))\n","  model_path = modeldir + \"-\" + suffix + \".h5\" # save format of model\n","  print(f\"Saving model to: {model_path}...\")\n","  model.save(model_path)\n","  return model_path"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"Y8u-65LpMt0w","executionInfo":{"status":"ok","timestamp":1669135891226,"user_tz":-330,"elapsed":19,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["def load_model(model_path):\n","  \"\"\"\n","  Loads a saved model from a specified path.\n","  \"\"\"\n","  print(f\"Loading saved model from: {model_path}\")\n","  model = tf.keras.models.load_model(model_path,\n","                                     custom_objects={\"KerasLayer\":hub.KerasLayer})\n","  return model"]},{"cell_type":"code","execution_count":73,"metadata":{"id":"02CIR-wyQ8sr","executionInfo":{"status":"ok","timestamp":1669135891227,"user_tz":-330,"elapsed":19,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# # Fit the full model to the full training data\n","# full_model2.fit(x=full_data,\n","#                epochs=5,\n","#                callbacks=[full_model_tensorboard, \n","#                           full_model_early_stopping, logger])"]},{"cell_type":"code","execution_count":74,"metadata":{"id":"HI4SVAQaMwXV","executionInfo":{"status":"ok","timestamp":1669135891227,"user_tz":-330,"elapsed":19,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# Save our model trained on 4000 images from the Training Dataset\n","# save_model(full_model2, suffix=\"res-net\")\n","\n","# x=save_model(full_model2, suffix=\"res-net\")"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"sCf_IBZ1BIlc","executionInfo":{"status":"ok","timestamp":1669135891227,"user_tz":-330,"elapsed":19,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# check train and test data size"]},{"cell_type":"code","execution_count":76,"metadata":{"id":"CNmB7eYIMywr","executionInfo":{"status":"ok","timestamp":1669135891228,"user_tz":-330,"elapsed":20,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# Load our model trained on 1000 images\n","\n","# loaded_model = load_model('models_archit/20221112-02251668219959-res-net.h5')\n","# loaded_model = load_model(x)"]},{"cell_type":"code","execution_count":77,"metadata":{"id":"VUgnLfpV9Wme","executionInfo":{"status":"ok","timestamp":1669135891228,"user_tz":-330,"elapsed":19,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"outputs":[],"source":["# X_val = X[:500]\n","# y_val = y[:500]\n","# val_data = create_data_batches(X_val, y_val, valid_data=True)"]},{"cell_type":"code","source":["full_data_test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5ePPG1jQ6nk","executionInfo":{"status":"ok","timestamp":1669136106437,"user_tz":-330,"elapsed":523,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"862034b5-4ece-40e4-c3f7-2455712d5382"},"execution_count":83,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<MapDataset element_spec=(TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(16,), dtype=tf.bool, name=None))>"]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uEKU-VFkdko2","executionInfo":{"status":"ok","timestamp":1669139527808,"user_tz":-330,"elapsed":449,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"1699299f-919f-4498-f823-7514ad97c255"},"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["'~'\t\t\t\t        mobile_net_l.png\n"," Datathon2.ipynb\t\t        models\n"," datathonindoml-2022.zip\t        models_archit\n"," densenet_a.png\t\t\t       'New Models.ipynb'\n"," dense-net-logger.csv\t\t        predicted_label_3.csv\n"," densenet_l.png\t\t\t        predicted_label.csv\n"," drive\t\t\t\t       'rekhani LSTM.ipynb'\n"," efficient_net_a.png\t\t        resnet_a.png\n"," efficient-net-logger.csv\t        res-net-logger.csv\n"," efficient_net_l.png\t\t        resnet_l.png\n","'few shot.ipynb'\t\t        sample_submission.csv\n","'for RESNETS.ipynb'\t\t        train\n"," full_np_array.npy\t\t        train_labels.csv\n"," kaggle-indoml-submission.csv\t        validation\n"," kaggle-indoml-submission-model-2.csv   vision_trans_a.png\n"," logs\t\t\t\t       'Vision Transformers.ipynb'\n"," mobile_net_a.png\t\t        vision-trans-logger.csv\n"," mobile-net-logger.csv\t\t        vision_trans_l.png\n"," mobile-net-logger.gsheet\n"]}]},{"cell_type":"code","execution_count":112,"metadata":{"id":"DgffU6nG94Mm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669140516488,"user_tz":-330,"elapsed":130781,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"a716b063-c2ab-4e29-e7c8-76e39d8b8059"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading saved model from: models_archit/20221112-15151668266108-res-net.h5\n","32/32 [==============================] - 15s 341ms/step - loss: 1.7276 - accuracy: 0.4530\n","Loading saved model from: models_archit/20221112-09411668246088-dense-net.h5\n","32/32 [==============================] - 8s 230ms/step - loss: 4.5251 - accuracy: 0.2220\n","Loading saved model from: models_archit/20221112-00031668211416-mobile-net.h5\n","32/32 [==============================] - 10s 284ms/step - loss: 1.2096 - accuracy: 0.6310\n","Loading saved model from: models/20221007-04581665118720-initial-4000-images-Adam.h5\n","32/32 [==============================] - 8s 230ms/step - loss: 1.5378 - accuracy: 0.5470\n","Loading saved model from: models/20221007-08281665131319-full-trained-adam.h5\n","32/32 [==============================] - 7s 216ms/step - loss: 0.9350 - accuracy: 0.7040\n","Loading saved model from: models/20221008-14551665240924-full-model-2-Adam.h5\n","32/32 [==============================] - 7s 209ms/step - loss: 0.8910 - accuracy: 0.7200\n","32/32 [==============================] - 7s 214ms/step\n","Loading saved model from: models_archit/20221112-02251668219959-efficient-net.h5\n","32/32 [==============================] - 7s 207ms/step - loss: 1.2357 - accuracy: 0.6060\n","32/32 [==============================] - 8s 219ms/step\n"]}],"source":["# # Evaluate the loaded model for all models\n","\n","# resnet_archit\n","loaded_model = load_model('models_archit/20221112-15151668266108-res-net.h5')\n","loaded_model.evaluate(full_data_test)\n","\n","# dense\n","loaded_model = load_model('models_archit/20221112-09411668246088-dense-net.h5')\n","loaded_model.evaluate(full_data_test)\n","\n","# mobile\n","loaded_model = load_model('models_archit/20221112-00031668211416-mobile-net.h5')\n","loaded_model.evaluate(full_data_test)\n","\n","# mobile_raj\n","loaded_model = load_model('models/20221007-04581665118720-initial-4000-images-Adam.h5')\n","loaded_model.evaluate(full_data_test)\n","\n","# mobile_raj_again\n","loaded_model = load_model('models/20221007-08281665131319-full-trained-adam.h5')\n","loaded_model.evaluate(full_data_test)\n","\n","# mobile_raj_again_again\n","loaded_model = load_model('models/20221008-14551665240924-full-model-2-Adam.h5')\n","loaded_model.evaluate(full_data_test)\n","predictions = loaded_model.predict(full_data_test, verbose=1) \n","predictions=pd.DataFrame(predictions)\n","predictions.to_csv('final_submit_approx_predicted_label_mobile-net-full.csv')\n","\n","# efficient\n","loaded_model = load_model('models_archit/20221112-02251668219959-efficient-net.h5')\n","loaded_model.evaluate(full_data_test)\n","predictions = loaded_model.predict(full_data_test, verbose=1) \n","predictions=pd.DataFrame(predictions)\n","predictions.to_csv('final_submit_approx_predicted_label_efficient-net.csv')\n"]},{"cell_type":"code","source":["# trans\n","loaded_model = load_model('models_archit/20221112-20131668284006-vision-trans.h5')\n","loaded_model.evaluate(full_data_test_2)\n","\n","predictions = loaded_model.predict(full_data_test_2, verbose=1) \n","predictions=pd.DataFrame(predictions)\n","predictions.to_csv('final_submit_approx_predicted_label_vision_trans.csv')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6bKIb8HTg6Q","executionInfo":{"status":"ok","timestamp":1669140545782,"user_tz":-330,"elapsed":29309,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}},"outputId":"bfe6a367-f820-4668-f155-2e42487369c3"},"execution_count":113,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading saved model from: models_archit/20221112-20131668284006-vision-trans.h5\n","32/32 [==============================] - 9s 237ms/step - loss: 1.5112 - accuracy: 0.7520\n","32/32 [==============================] - 9s 239ms/step\n"]}]},{"cell_type":"code","source":["\n","p1=pd.read_csv('final_submit_approx_predicted_label_efficient-net.csv').drop(['Unnamed: 0'],axis=1)\n","p2=pd.read_csv('final_submit_approx_predicted_label_mobile-net-full.csv').drop(['Unnamed: 0'],axis=1)\n","p3=pd.read_csv('final_submit_approx_predicted_label_vision_trans.csv').drop(['Unnamed: 0'],axis=1)\n"],"metadata":{"id":"2rOL-I2lfbMY","executionInfo":{"status":"ok","timestamp":1669140804269,"user_tz":-330,"elapsed":4,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"execution_count":122,"outputs":[]},{"cell_type":"code","source":["p_loss=p1*0.6060+p2*0.7200+p3*0.7520/(0.6060+0.7200+0.7520)\n","p_acc=(p1*(1/1.2357)+p2*(1/0.8910)+p3*(1/1.5112))/((1/1.2357)+(1/0.8910)+(1/1.5112))"],"metadata":{"id":"CfIDBPq1iD1u","executionInfo":{"status":"ok","timestamp":1669141408184,"user_tz":-330,"elapsed":506,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"execution_count":124,"outputs":[]},{"cell_type":"code","source":["p_loss\n","p_loss.to_csv('ensemble_loss.csv')"],"metadata":{"id":"BHPBuZKylLIQ","executionInfo":{"status":"ok","timestamp":1669141527394,"user_tz":-330,"elapsed":777,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"execution_count":127,"outputs":[]},{"cell_type":"code","source":["loss=tf.keras.losses.CategoricalCrossentropy()"],"metadata":{"id":"vnAZsSQZl5cy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p_acc\n","p_acc.to_csv('ensemble_accuracy.csv')"],"metadata":{"id":"0eB9NxEClNXq","executionInfo":{"status":"ok","timestamp":1669141540564,"user_tz":-330,"elapsed":399,"user":{"displayName":"Archit Mangrulkar","userId":"15077082084364612857"}}},"execution_count":128,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tslYVCvFZTfu"},"outputs":[],"source":["# predictions = loaded_model.predict(val_data, verbose=1) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m81HAQAMZuVF"},"outputs":[],"source":["# predictions.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYdDjvY2aFNE"},"outputs":[],"source":["# predictions = loaded_model.predict(val_data, verbose=1) \n","# predictions=pd.DataFrame(predictions)\n","# predictions.to_csv('drive/MyDrive/Datathon/approx_predicted_label_res-net.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bfQwWQmi-9ee"},"outputs":[],"source":["# # Turn prediction probabilities into their labels (Document Types)\n","# def get_pred_label(prediction_probabilities):\n","#   \"\"\"\n","#   Turns an array of prediction probabilities into a label.\n","#   \"\"\"\n","#   return unique_labels[np.argmax(prediction_probabilities)]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jh7rKKQFGefG"},"outputs":[],"source":["# model_path = \"models_archit/20221112-00031668211416-efficient-net.h5\" \n","# data_path = \"validation/validation\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7U88mvoxGc6q"},"outputs":[],"source":["# # Function to test the validation data stored in 'data_path' with the model stored in 'model_path'\n","# # here, model_path = \"drive/MyDrive/Datathon/models/20221007-08281665131319-full-trained-adam.h5\" \n","# #       data_path = \"drive/MyDrive/Datathon/validation/validation\"\n","\n","# def test(model_path, data_path):\n","#   # Load the fully trained model\n","#   loaded_full_model = load_model(model_path)\n","\n","#   # Load validation image filenames\n","#   val_path = data_path\n","#   val_filenames = [val_path + fname for fname in os.listdir(val_path)]\n","\n","#   # Getting the list of validation set IDs\n","#   val_id = [id for id in os.listdir(val_path)]\n","#   val_ids = []\n","#   for item in val_id:\n","#     val_ids.append(int(item.split(\".\")[0]))\n","  \n","#   # Create validation data batch so as to turn it into tensors and then fit it in our model\n","#   val_data = create_data_batches(val_filenames, test_data=True) \n","\n","#   # Make predictions on the validation data \n","#   predictions = loaded_full_model.predict(val_data, verbose=1) \n","  \n","#   # Getting the predicted labels in array val_pred_labels[]\n","#   val_pred_labels = []\n","#   for i in range(len(val_ids)):\n","#     val_pred_labels.append(get_pred_label(predictions[i]))\n","  \n","#   # Fitting the data into Pandas dataframe\n","#   data = []\n","#   for i in range(len(val_ids)):\n","#     data.append((val_ids[i], val_pred_labels[i]))\n","#   df = pd.DataFrame(data, columns=['id','label'])\n","\n","#   # Saving the predicted labels on validation set images in CSV\n","#   # Saving the predictions to predicted_label.csv file and saving it inside the datathon folder in GDrive\n","#   # df.to_csv(r'drive/MyDrive/Datathon/predicted_label2.csv', index=False)\n","\n","#   df.to_csv(r'predicted_label_mobile-net.csv', index=False)  \n","#   # df.to_csv(r'predicted_label_efficient-net.csv', index=False)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhmWn7z10bh3"},"outputs":[],"source":["# test(model_path, data_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W48rem9SGj9L"},"outputs":[],"source":["# data= pd.read_csv('drive/MyDrive/Datathon/predicted_label.csv')\n","# data"]},{"cell_type":"markdown","metadata":{"id":"mOOROZC-R3mO"},"source":["# Few Shot Image Classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"US0foN1BR5dW"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import layers\n","import tensorflow_datasets as tfds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFwsqnH7TF6H"},"outputs":[],"source":["learning_rate = 0.003\n","meta_step_size = 0.25\n","\n","inner_batch_size = 25\n","eval_batch_size = 25\n","\n","meta_iters = 500\n","eval_iters = 5\n","inner_iters = 4\n","\n","eval_interval = 1\n","train_shots = 20\n","shots = 5\n","classes = 16"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-epJCrZoGdj"},"outputs":[],"source":["# # class Dataset:\n","# #     # This class will facilitate the creation of a few-shot dataset\n","# #     # from the Omniglot dataset that can be sampled from quickly while also\n","# #     # allowing to create new labels at the same time.\n","# #     def __init__(self, training):\n","# #         # Download the tfrecord files containing the omniglot data and convert to a\n","# #         # dataset.\n","# #         split = \"train\" if training else \"test\"\n","# #         ds = tfds.load(\"omniglot\", split=split, as_supervised=True, shuffle_files=False)\n","# #         # Iterate over the dataset to get each individual image and its class,\n","# #         # and put that data into a dictionary.\n","# #         self.data = {}\n","\n","# #         def extraction(image, label):\n","# #             # This function will shrink the Omniglot images to the desired size,\n","# #             # scale pixel values and convert the RGB image to grayscale\n","# #             image = tf.image.convert_image_dtype(image, tf.float32)\n","# #             # image = tf.image.rgb_to_grayscale(image)\n","# #             image = tf.image.resize(image, [224, 224, 3])\n","# #             return image, label\n","\n","# #         for image, label in ds.map(extraction):\n","# #             image = image.numpy()\n","# #             label = str(label.numpy())\n","# #             if label not in self.data:\n","# #                 self.data[label] = []\n","# #             self.data[label].append(image)\n","# #         self.labels = list(self.data.keys())\n","\n","# def get_mini_dataset(, batch_size, repetitions, shots, num_classes, split=False):\n","#   temp_labels = np.zeros(shape=(num_classes * shots))\n","#   temp_images = np.zeros(shape=(num_classes * shots, 224, 224, 3))\n","\n","#   # Get a random subset of labels from the entire label set.\n","#   label_subset = random.choices(self.labels, k=num_classes)\n","#   for class_idx, class_obj in enumerate(label_subset):\n","#     # Use enumerated index value as a temporary label for mini-batch in\n","#     # few shot learning.\n","#     temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx\n","#     # If creating a split dataset for testing, select an extra sample from each\n","#     # label to create the test dataset.\n","#     # For each index in the randomly selected label_subset, sample the\n","#     # necessary number of images.\n","#     temp_images[\n","#         class_idx * shots : (class_idx + 1) * shots\n","#     ] = random.choices(self.data[label_subset[class_idx]], k=shots)\n","\n","#   dataset = tf.data.Dataset.from_tensor_slices(\n","#     (temp_images.astype(np.float32), temp_labels.astype(np.int32))\n","#   )\n","#   dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)\n","#   return dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1669064651150,"user":{"displayName":"totally reliable","userId":"15953242274047744585"},"user_tz":-330},"id":"rzuuohhHDge0","outputId":"884b6cd0-2683-4570-fcf2-485c2549cb68"},"outputs":[{"data":{"text/plain":["<MapDataset element_spec=(TensorSpec(shape=(224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(16,), dtype=tf.bool, name=None))>"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["full_data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1669064651150,"user":{"displayName":"totally reliable","userId":"15953242274047744585"},"user_tz":-330},"id":"cQVgqT9-_Vq1","outputId":"6db520af-f09d-4f35-c180-1c05310515df"},"outputs":[{"data":{"text/plain":["<PrefetchDataset element_spec={'alphabet': TensorSpec(shape=(), dtype=tf.int64, name=None), 'alphabet_char_id': TensorSpec(shape=(), dtype=tf.int64, name=None), 'image': TensorSpec(shape=(105, 105, 3), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["ds = tfds.load(\"omniglot\")\n","ds['train']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPlhGol545qu"},"outputs":[],"source":["class Dataset:\n","    # This class will facilitate the creation of a few-shot dataset\n","    # from the Omniglot dataset that can be sampled from quickly while also\n","    # allowing to create new labels at the same time.\n","    def __init__(self, training):\n","        # Download the tfrecord files containing the omniglot data and convert to a\n","        # dataset.\n","        split = \"train\" if training else \"test\"\n","        # ds = tfds.load(\"omniglot\", split=split, as_supervised=True, shuffle_files=False)\n","        ds=full_data\n","        # Iterate over the dataset to get each individual image and its class,\n","        # and put that data into a dictionary.\n","        self.data = {}\n","        for i in range(0,16):\n","                self.data[i]=[]\n","\n","        def extraction(image, label):\n","            # This function will shrink the Omniglot images to the desired size,\n","            # scale pixel values and convert the RGB image to grayscale\n","            # image = tf.image.convert_image_dtype(image, tf.float32)\n","            # image = tf.image.resize(image, [224, 224, 3])\n","            return image, label\n","\n","        for image, label in ds.map(extraction):\n","            image = image.numpy()\n","            label = label.numpy()\n","            # print(label.shape)\n","            for i in range(0,16):\n","              if label[i]==1:\n","                label_z=i\n","            self.data[label_z].append(image)\n","        self.labels = list(self.data.keys())\n","\n","    def get_mini_dataset(\n","        self, batch_size, repetitions, shots, num_classes, split=False\n","    ):\n","        temp_labels = np.zeros(shape=(num_classes * shots, 16))\n","        temp_images = np.zeros(shape=(num_classes * shots, 224, 224, 3))\n","        if split:\n","            test_labels = np.zeros(shape=(num_classes, 16))\n","            test_images = np.zeros(shape=(num_classes, 224, 224, 3))\n","\n","        # Get a random subset of labels from the entire label set.\n","        label_subset = random.choices(self.labels, k=num_classes)\n","        for class_idx, class_obj in enumerate(label_subset):\n","            # Use enumerated index value as a temporary label for mini-batch in\n","            # few shot learning.\n","            class_idx_vec = np.zeros(shape=(16))\n","            class_idx_vec[class_idx]=1\n","            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx_vec\n","            # If creating a split dataset for testing, select an extra sample from each\n","            # label to create the test dataset.\n","            if split:\n","                test_labels[class_idx] = class_idx_vec\n","                images_to_split = random.choices(\n","                    self.data[label_subset[class_idx]], k=shots + 1\n","                )\n","                test_images[class_idx] = images_to_split[-1]\n","                temp_images[\n","                    class_idx * shots : (class_idx + 1) * shots\n","                ] = images_to_split[:-1]\n","            else:\n","                # For each index in the randomly selected label_subset, sample the\n","                # necessary number of images.\n","                temp_images[\n","                    class_idx * shots : (class_idx + 1) * shots\n","                ] = random.choices(self.data[label_subset[class_idx]], k=shots)\n","\n","        dataset = tf.data.Dataset.from_tensor_slices(\n","            (temp_images.astype(np.float32), temp_labels.astype(np.int32))\n","        )\n","        dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)\n","        if split:\n","            return dataset, test_images, test_labels\n","        return dataset\n","\n","\n","import urllib3\n","\n","urllib3.disable_warnings()  # Disable SSL warnings that may happen during download.\n","train_dataset = Dataset(training=True)\n","# test_dataset = Dataset(training=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2j-jTS7CTWmn"},"outputs":[],"source":["def conv_bn(x):\n","    x = layers.Conv2D(filters=64, kernel_size=3, strides=2, padding=\"same\")(x)\n","    x = layers.BatchNormalization()(x)\n","    return layers.ReLU()(x)\n","\n","\n","inputs = layers.Input(shape=(224, 224, 3))\n","x = conv_bn(inputs)\n","x = conv_bn(x)\n","x = conv_bn(x)\n","x = conv_bn(x)\n","x = layers.Flatten()(x)\n","outputs = layers.Dense(16, activation=\"softmax\")(x)\n","model = keras.Model(inputs=inputs, outputs=outputs)\n","model.compile()\n","optimizer = keras.optimizers.SGD(learning_rate=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1669064696587,"user":{"displayName":"totally reliable","userId":"15953242274047744585"},"user_tz":-330},"id":"TJhiunoSwcvo","outputId":"eb75d2a6-0417-417a-f3d1-224d410105b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 112, 112, 64)      1792      \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 112, 112, 64)     256       \n"," hNormalization)                                                 \n","                                                                 \n"," re_lu_4 (ReLU)              (None, 112, 112, 64)      0         \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 56, 56, 64)        36928     \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 56, 56, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," re_lu_5 (ReLU)              (None, 56, 56, 64)        0         \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 28, 28, 64)        36928     \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 28, 28, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," re_lu_6 (ReLU)              (None, 28, 28, 64)        0         \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 14, 14, 64)        36928     \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 14, 14, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," re_lu_7 (ReLU)              (None, 14, 14, 64)        0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 12544)             0         \n","                                                                 \n"," dense_1 (Dense)             (None, 16)                200720    \n","                                                                 \n","=================================================================\n","Total params: 314,320\n","Trainable params: 313,808\n","Non-trainable params: 512\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"executionInfo":{"elapsed":35187,"status":"error","timestamp":1669067772520,"user":{"displayName":"totally reliable","userId":"15953242274047744585"},"user_tz":-330},"id":"kNTm-730TXX6","outputId":"79e17f7d-e55f-4e71-8c43-4159379e86b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["batch 0: train=%93.75\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-133-4c4dd858ab00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# print(mini_dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m           output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3012\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   3013\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m         \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   3015\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["training = []\n","testing = []\n","\n","# meta_iters=5\n","for meta_iter in range(meta_iters):\n","    frac_done = meta_iter / meta_iters\n","    cur_meta_step_size = (1 - frac_done) * meta_step_size\n","    # Temporarily save the weights from the model.\n","    old_vars = model.get_weights()\n","    # Get a sample from the full dataset.\n","    mini_dataset = train_dataset.get_mini_dataset(\n","        inner_batch_size, inner_iters, train_shots, classes\n","    )\n","    # print(mini_dataset)\n","    for images, labels in mini_dataset:\n","        with tf.GradientTape() as tape:\n","            preds = model(images)\n","            labels=tf.reshape(labels, preds.shape)\n","            # loss = keras.losses.categorical_crossentropy(tf.reshape(labels, [25,16]), tf.reshape(preds, [25,16]))\n","            loss = keras.losses.categorical_crossentropy(labels, preds)\n","        grads = tape.gradient(loss, model.trainable_weights)\n","        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","    new_vars = model.get_weights()\n","    # Perform SGD for the meta step.\n","    for var in range(len(new_vars)):\n","        new_vars[var] = old_vars[var] + (\n","            (new_vars[var] - old_vars[var]) * cur_meta_step_size\n","        )\n","        # print(f\"difference = {new_vars[var] - old_vars[var]}\")\n","    # After the meta-learning step, reload the newly-trained weights into the model.\n","    model.set_weights(new_vars)\n","    # Evaluation loop\n","    if meta_iter % eval_interval == 0:\n","        accuracies = []\n","        # for dataset in (train_dataset, test_dataset):\n","            # Sample a mini dataset from the full dataset.\n","        train_set = train_dataset.get_mini_dataset(\n","            eval_batch_size, eval_iters, shots, classes, split=False\n","        )\n","        old_vars = model.get_weights()\n","        # Train on the samples and get the resulting accuracies.\n","        # print(train_set)\n","        for images, labels in train_set:\n","            with tf.GradientTape() as tape:\n","                preds = model(images)\n","                loss = keras.losses.categorical_crossentropy(labels, preds)\n","            grads = tape.gradient(loss, model.trainable_weights)\n","            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","        # train_preds = model.predict(images)\n","        # train_preds = tf.argmax(preds).numpy()\n","        # print(labels.shape)\n","        # print(preds.shape)\n","        num_correct=0\n","        for x in range(labels.shape[0]):\n","          for y in range(labels.shape[1]):\n","            demo=0\n","            if preds[x][y] > 0.5:\n","              demo=1\n","            if(labels[x][y]==demo):\n","              num_correct+=1\n","        acc=num_correct/80\n","        # num_correct = (train_preds == labels).sum()\n","        # Reset the weights after getting the evaluation accuracies.\n","        model.set_weights(old_vars)\n","        accuracies.append(acc)\n","        # training.append(accuracies[0])\n","        if meta_iter % 10 == 0:\n","            print(\n","                f\"batch {meta_iter}: train=%{100*acc}\"\n","            )"]},{"cell_type":"code","source":["full_data_test = create_data_batches(X_test, y_test, 1000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ccYToMDyQt3y","executionInfo":{"status":"ok","timestamp":1669068954653,"user_tz":-330,"elapsed":16,"user":{"displayName":"totally reliable","userId":"15953242274047744585"}},"outputId":"65ddcf92-1d31-47a6-cf85-f33d1c9372f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating training data batches...\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3OYVXZyiNELF"},"outputs":[],"source":["class Dataset:\n","    # This class will facilitate the creation of a few-shot dataset\n","    # from the Omniglot dataset that can be sampled from quickly while also\n","    # allowing to create new labels at the same time.\n","    def __init__(self, training):\n","        # Download the tfrecord files containing the omniglot data and convert to a\n","        # dataset.\n","        split = \"train\" if training else \"test\"\n","        # ds = tfds.load(\"omniglot\", split=split, as_supervised=True, shuffle_files=False)\n","        ds=full_data_test\n","        # Iterate over the dataset to get each individual image and its class,\n","        # and put that data into a dictionary.\n","        self.data = {}\n","        for i in range(0,16):\n","                self.data[i]=[]\n","\n","        def extraction(image, label):\n","            # This function will shrink the Omniglot images to the desired size,\n","            # scale pixel values and convert the RGB image to grayscale\n","            # image = tf.image.convert_image_dtype(image, tf.float32)\n","            # image = tf.image.resize(image, [224, 224, 3])\n","            return image, label\n","\n","        for image, label in ds.map(extraction):\n","            image = image.numpy()\n","            label = label.numpy()\n","            # print(label.shape)\n","            for i in range(0,16):\n","              if label[i]==1:\n","                label_z=i\n","            self.data[label_z].append(image)\n","        self.labels = list(self.data.keys())\n","\n","    def get_mini_dataset(\n","        self, batch_size, repetitions, shots, num_classes, split=False\n","    ):\n","        temp_labels = np.zeros(shape=(num_classes * shots, 16))\n","        temp_images = np.zeros(shape=(num_classes * shots, 224, 224, 3))\n","        if split:\n","            test_labels = np.zeros(shape=(num_classes, 16))\n","            test_images = np.zeros(shape=(num_classes, 224, 224, 3))\n","\n","        # Get a random subset of labels from the entire label set.\n","        label_subset = random.choices(self.labels, k=num_classes)\n","        for class_idx, class_obj in enumerate(label_subset):\n","            # Use enumerated index value as a temporary label for mini-batch in\n","            # few shot learning.\n","            class_idx_vec = np.zeros(shape=(16))\n","            class_idx_vec[class_idx]=1\n","            temp_labels[class_idx * shots : (class_idx + 1) * shots] = class_idx_vec\n","            # If creating a split dataset for testing, select an extra sample from each\n","            # label to create the test dataset.\n","            if split:\n","                test_labels[class_idx] = class_idx_vec\n","                images_to_split = random.choices(\n","                    self.data[label_subset[class_idx]], k=shots + 1\n","                )\n","                test_images[class_idx] = images_to_split[-1]\n","                temp_images[\n","                    class_idx * shots : (class_idx + 1) * shots\n","                ] = images_to_split[:-1]\n","            else:\n","                # For each index in the randomly selected label_subset, sample the\n","                # necessary number of images.\n","                temp_images[\n","                    class_idx * shots : (class_idx + 1) * shots\n","                ] = random.choices(self.data[label_subset[class_idx]], k=shots)\n","\n","        dataset = tf.data.Dataset.from_tensor_slices(\n","            (temp_images.astype(np.float32), temp_labels.astype(np.int32))\n","        )\n","        dataset = dataset.shuffle(100).batch(batch_size).repeat(repetitions)\n","        if split:\n","            return dataset, test_images, test_labels\n","        return dataset\n","\n","\n","import urllib3\n","\n","urllib3.disable_warnings()  # Disable SSL warnings that may happen during download.\n","test_dataset = Dataset(training=True)\n","# test_dataset = Dataset(training=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PxFnT6pWMKoa"},"outputs":[],"source":["if meta_iter % eval_interval == 0:\n","        accuracies = []\n","        # for dataset in (train_dataset, test_dataset):\n","            # Sample a mini dataset from the full dataset.\n","        test_set = test_dataset.get_mini_dataset(\n","            eval_batch_size, eval_iters, shots, classes, split=False\n","        )\n","        old_vars = model.get_weights()\n","        # Train on the samples and get the resulting accuracies.\n","        # print(train_set)\n","        for images, labels in test_set:\n","            with tf.GradientTape() as tape:\n","                preds = model(images)\n","                loss = keras.losses.categorical_crossentropy(labels, preds)\n","            grads = tape.gradient(loss, model.trainable_weights)\n","            optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","        # train_preds = model.predict(images)\n","        # train_preds = tf.argmax(preds).numpy()\n","        # print(labels.shape)\n","        # print(preds.shape)\n","        num_correct=0\n","        for x in range(labels.shape[0]):\n","          for y in range(labels.shape[1]):\n","            demo=0\n","            if preds[x][y] > 0.5:\n","              demo=1\n","            if(labels[x][y]==demo):\n","              num_correct+=1\n","        acc=num_correct/80\n","        # num_correct = (train_preds == labels).sum()\n","        # Reset the weights after getting the evaluation accuracies.\n","        model.set_weights(old_vars)\n","        accuracies.append(acc)\n","        # training.append(accuracies[0])\n","        if meta_iter % 10 == 0:\n","            print(\n","                f\"batch {meta_iter}: train=%{100*acc}\"\n","            )"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}